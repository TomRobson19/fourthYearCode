#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\date{}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip bigskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Multimedia Technologies Summative Assignment
\end_layout

\begin_layout Author
hzwr87
\end_layout

\begin_layout Section
Describe the main difference between the Human “Earscape” model (Ref: Lecture
 note 2) and the Audio Noise Masking model (Ref: MP3).
 Explain how these two models are applied to optimize signal quantization
 in the audio digitalization process.
 [20 Marks] 
\end_layout

\begin_layout Standard
Signal Quantisation is the part of the audio digitalisation process that
 divides the signal strength into discrete levels.
 The Human Earscape model defines what kinds of information that the human
 ear is capable of perceiving, and what level of sound pressure is required
 to enable us to perceive specific frequencies.
 The range of the earscape model is bounded at the low sound pressure end
 of the spectrum by the Miniumum Audability curve.
 This tells us that low frequency sounds can only be heard at higher sound
 pressure levels, whereas higher frequency sounds can be heard at comparatively
 lower sound pressure levels.
 The upper bound of this model is the Terminal Threshold, which is the sound
 pressure level as which the sound is not percieved to be any louder, and
 can cause pain and discomfort to the listener.
 This earscape model is defined for the average human listener and is used
 to allow compression algorithms to vary across different frequency levels.
\end_layout

\begin_layout Standard
The earscape model can tell us the appropriate frequency range for different
 types of content, and quantisation algorithms can therefore remove information
 outside of these frequency ranges.
 For example, a voice recording file could have all frequencies outside
 of the typical human vocal range removed in order to reduce file size and
 aid in the understandability of the file.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Earscape - low frequency have to be louder.
 higher frequencies can be anything.
 can use this as part of the encoding process to do some compression.
 
\end_layout

\begin_layout Plain Layout
Audio Noise Masking - playing one sound over another means you can't hear
 the first sound as well.
 Adding another sound in the same frequency band 
\begin_inset Quotes eld
\end_inset

masks
\begin_inset Quotes erd
\end_inset

 it.
 Can use in the MP3 algorithm to work out when it can get away with doing
 more quantisation by adding more noise as this is being masked by the original
 signal.
 
\end_layout

\end_inset


\end_layout

\begin_layout Section
MPEG/Audio Layer I encoding divides an input audio signal into 32 sub-bands
 forming the inputs for audio digitalization.
 Justify whether this procedure improves or degrades audio signal quantization.
 (Ref: MP3) [20 Marks] 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Don't compare to other numbers of bands
\end_layout

\end_inset


\end_layout

\begin_layout Section
In your own words, explain the purpose of involving Discrete Cosine Transform
 (DCT) in MPEG/Audio Layer 3.
 Justify how this procedure improves the quality of audio digitalization
 in terms of both the audio signal preserved and file size produced.
 (Ref: MP3) [20 Marks] 
\end_layout

\begin_layout Section
Justify the suitability of replacing the DCT function in MPEG/Audio Layer
 3 with Wavelet transform, explaining any requirement in choosing Wavelet
 transform functions and what changes have to make in MPEG/Audio Layer 3
 components to allow such a replacement.
 (Ref: Lecture note and MP3) [20 Marks] 
\end_layout

\begin_layout Section
In your own words, explain four differences between applying DCT and Wavelet
 transform to compress an image, and justify their difference in supporting
 image decompression and transmission.
 (Ref: Lecture note) [20 Marks] 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Wavelets are lecture 4
\end_layout

\end_inset


\end_layout

\begin_layout Section
Level 4 students - Conducting a research in scalable video coding (SVC)
 in the H.264/AVC standard, write up your finding about how SVC is technically
 different from the original H.264/AVC standard, in terms of architecture,
 coding mechanism, functionalities, video representation and transmission.
 (Ref: Lecture note and SVC) [50 Marks] 
\end_layout

\end_body
\end_document
