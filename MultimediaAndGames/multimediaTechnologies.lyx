#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\date{}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip bigskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Multimedia Technologies Summative Assignment
\end_layout

\begin_layout Author
hzwr87
\end_layout

\begin_layout Section
Describe the main difference between the Human “Earscape” model (Ref: Lecture
 note 2) and the Audio Noise Masking model (Ref: MP3).
 Explain how these two models are applied to optimize signal quantization
 in the audio digitalization process.
 [20 Marks] 
\end_layout

\begin_layout Standard
The Human Earscape model defines what kinds of information that the human
 ear is capable of perceiving, and what level of sound pressure is required
 to enable us to perceive specific frequencies.
 The range of the earscape model is bounded at the low sound pressure end
 of the spectrum by the Miniumum Audability curve.
 This tells us that low frequency sounds can only be heard at higher sound
 pressure levels, whereas higher frequency sounds can be heard at comparatively
 lower sound pressure levels.
 The upper bound of this model is the Terminal Threshold, which is the sound
 pressure level as which the sound is not percieved to be any louder, and
 can cause pain and discomfort to the listener.
 This earscape model is defined for the average human listener and is used
 to allow compression algorithms to vary across different frequency levels.
 This model can tell us the appropriate frequency range for different types
 of content, and quantisation algorithms can therefore be optimised to remove
 information outside of these frequency ranges.
 For example, a voice recording file could have all frequencies outside
 of the typical human vocal range removed in order to simplify the audio
 digitialisation process and eventually reduce the file size.
 
\end_layout

\begin_layout Standard
The audio masking model in the MP3 standard takes advantage of a perceptual
 weakness of human hearing that occurs when a strong audio signal of a certain
 frequency obscures, or 'masks', weaker audio signals of similar frequencies.
 To take advantage of this, we can partition the audible spectrum into bands
 based on how capable the ear is of perceiving different frequencies.
 The purpose of this partitioning is to understand the size of the neighbourhood
 of frequencies that will be 'masked' by a strong audio signal.
 This can be taken advantage of in the signal quantisation process by first
 transforming the audio signal into the frequency domain, and breaking this
 down into subbands that represent the neighbourhoods in which masking will
 occur enables the MP3 algorithm to set the quantisation level to the maximum
 possible value such that the level of quantisation noise will be imperceptible
 to the listener.
 Masking can also occur temporally, as experiments have shown that it takes
 some time for us to be able to perceive a weak signal after hearing a strong
 signal at a similar frequency.
 The MP3 algorithm uses this to further quantise the signal, by determining
 whether a sound will be heard or not following a strong sound based on
 its frequency, sound pressure and the time between the two sounds.
\end_layout

\begin_layout Section
MPEG/Audio Layer I encoding divides an input audio signal into 32 sub-bands
 forming the inputs for audio digitalization.
 Justify whether this procedure improves or degrades audio signal quantization.
 (Ref: MP3) [20 Marks] 
\end_layout

\begin_layout Standard
This division is performed in MPEG layer 1 by a basic filter bank.
 The 32 subbands are constant width, whereas the ability of humans to tell
 the difference between sounds of different frequencies, known as the critical
 bands of the ear, are not of constant width but instead vary as different
 frequency levels.
 Therefore, these 32 bands do not accurately reflect the ears critical bands.
 The bandwidth is too wide at the lower frequencies, which prevents the
 specific tuning of quantiser bits to the actual critical bands.
 Furthermore, this filter bank and its inverse are not lossless transformations,
 as even if no quantisation were to take place the inverse transform would
 not perfectly recover the original input signal.
 Also, adjacent filter bands have a considerable frequency overlap, meaning
 that a signal at a single frequency can affect two adjacent filter bank
 outputs.
 From these 32 bands, the layer 1 algorithm groups together 12 samples from
 each of the bands, and allocates them a number of bits to be used for represent
ation, given by a scale factor which attempts to maximise the resolution
 of the quantiser.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
DOES THIS IMPROVE OR DEGRADE??
\end_layout

\end_inset


\end_layout

\begin_layout Section
In your own words, explain the purpose of involving Discrete Cosine Transform
 (DCT) in MPEG/Audio Layer 3.
 Justify how this procedure improves the quality of audio digitalization
 in terms of both the audio signal preserved and file size produced.
 (Ref: MP3) [20 Marks] 
\end_layout

\begin_layout Standard
The layer 3 algorithm is a far more refined and complex approach to audio
 digitalisation than the layer 1 and layer 2 algorithm.
 
\end_layout

\begin_layout Section
Justify the suitability of replacing the DCT function in MPEG/Audio Layer
 3 with Wavelet transform, explaining any requirement in choosing Wavelet
 transform functions and what changes have to make in MPEG/Audio Layer 3
 components to allow such a replacement.
 (Ref: Lecture note and MP3) [20 Marks] 
\end_layout

\begin_layout Section
In your own words, explain four differences between applying DCT and Wavelet
 transform to compress an image, and justify their difference in supporting
 image decompression and transmission.
 (Ref: Lecture note) [20 Marks] 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Wavelets are lecture 4 - roughly 20 mins in to recording
\end_layout

\end_inset


\end_layout

\begin_layout Section
Level 4 students - Conducting a research in scalable video coding (SVC)
 in the H.264/AVC standard, write up your finding about how SVC is technically
 different from the original H.264/AVC standard, in terms of architecture,
 coding mechanism, functionalities, video representation and transmission.
 (Ref: Lecture note and SVC) [50 Marks] 
\end_layout

\end_body
\end_document
