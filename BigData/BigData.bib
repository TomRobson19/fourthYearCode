@misc{Pennington2014,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
title = {{GloVe: Global Vectors for Word Representation}},
url = {https://nlp.stanford.edu/projects/glove/},
urldate = {2018-03-09},
year = {2014}
}
@inproceedings{Pennington2014a,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/tom/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
pages = {1532--1543},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
url = {https://nlp.stanford.edu/pubs/glove.pdf http://aclweb.org/anthology/D14-1162},
year = {2014}
}
@misc{FrancoisChollet2016,
author = {{Francois Chollet}},
booktitle = {The Keras Blog},
title = {{Using pre-trained word embeddings in a Keras model}},
url = {https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html},
urldate = {2018-03-09},
year = {2016}
}
